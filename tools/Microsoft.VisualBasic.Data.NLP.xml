<?xml version="1.0"?>
<doc>
<assembly>
<name>
Microsoft.VisualBasic.Data.NLP
</name>
</assembly>
<members>
<member name="T:Microsoft.VisualBasic.Data.NLP.My.Resources.Resources">
<summary>
  A strongly-typed resource class, for looking up localized strings, etc.
</summary>
</member>
<member name="P:Microsoft.VisualBasic.Data.NLP.My.Resources.Resources.ResourceManager">
<summary>
  Returns the cached ResourceManager instance used by this class.
</summary>
</member>
<member name="P:Microsoft.VisualBasic.Data.NLP.My.Resources.Resources.Culture">
<summary>
  Overrides the current thread's CurrentUICulture property for all
  resource lookups using this strongly typed resource class.
</summary>
</member>
<member name="T:Microsoft.VisualBasic.Data.NLP.LDA.Corpus">
 <summary>
 a set of documents
 语料库，也就是文档集合
 
 @author hankcs
 </summary>
</member>
<member name="M:Microsoft.VisualBasic.Data.NLP.LDA.Corpus.addDocument(System.Collections.Generic.IEnumerable{System.String})">
 <summary>
 a collection of the words
 </summary>
 <param name="document"></param>
 <returns></returns>
</member>
<member name="M:Microsoft.VisualBasic.Data.NLP.LDA.Corpus.load(System.String)">
 <summary>
 Load documents from disk
 </summary>
 <param name="folderPath"> is a folder, which contains text documents. </param>
 <returns> a corpus </returns>
 <exception cref="T:System.IO.IOException"> </exception>
</member>
<member name="M:Microsoft.VisualBasic.Data.NLP.LDA.Debugger.inference(System.Double,System.Double,System.Double[][],System.Int32[])">
 <summary>
 Inference a new document by a pre-trained phi matrix
 </summary>
 <param name="phi"> pre-trained phi matrix </param>
 <param name="doc"> document </param>
 <returns> a p array </returns>
</member>
<member name="M:Microsoft.VisualBasic.Data.NLP.LDA.Debugger.shadeDouble(System.Double,System.Double)">
 <summary>
 create a string representation whose gray value appears as an indicator
 of magnitude, cf. Hinton diagrams in statistics.
 </summary>
 <param name="d">   value </param>
 <param name="max"> maximum value
 @return </param>
</member>
<member name="M:Microsoft.VisualBasic.Data.NLP.LDA.Debugger.hist(System.Double[],System.Int32)">
 <summary>
 Print table of multinomial data
 </summary>
 <param name="data"> vector of evidence </param>
 <param name="fmax"> max frequency in display </param>
 <return> the scaled histogram bin values </return>
</member>
<member name="M:Microsoft.VisualBasic.Data.NLP.LDA.DocumentLoader.load(System.String)">
 <summary>
 Load documents from disk
 </summary>
 <param name="folderPath"> is a folder, which contains text documents. </param>
 <returns> a corpus </returns>
 <exception cref="T:System.IO.IOException"> </exception>
</member>
<member name="M:Microsoft.VisualBasic.Data.NLP.LDA.GibbsSamplingTask.gibbs_sampling(System.Int32,System.Int32,System.Int32[]@,System.Int32[]@,System.Int32[]@,System.Int32[]@,Microsoft.VisualBasic.Data.NLP.LDA.GibbsSamplingTask.gibbs_pars@)">
 <summary>
 Sample a topic z_i from the full conditional distribution: p(z_i = j |
 z_-i, w) = (n_-i,j(w_i) + beta)/(n_-i,j(.) + W * beta) * (n_-i,j(d_i) +
 alpha)/(n_-i,.(d_i) + K * alpha) 
 </summary>
 <param name="m"> document </param>
 <returns>
 sampling and assign new topic index
 </returns>
 <remarks>
 根据上述公式计算文档m中第n个词语的主题的完全条件分布，输出最可能的主题
 </remarks>
</member>
<member name="T:Microsoft.VisualBasic.Data.NLP.LDA.LdaGibbsSampler">
 <summary>
 Gibbs sampler for estimating the best assignments of topics for words and
 documents in a corpus. The algorithm is introduced in Tom Griffiths' paper
 "Gibbs sampling in the generative model of Latent Dirichlet Allocation"
 (2002).
 Gibbs sampler采样算法的实现
 
 @author heinrich
 </summary>
 <remarks>
 https://github.com/hankcs/LDA4j
 </remarks>
</member>
<member name="F:Microsoft.VisualBasic.Data.NLP.LDA.LdaGibbsSampler.documents">
 <summary>
 document data (term lists)
 文档
 </summary>  
</member>
<member name="F:Microsoft.VisualBasic.Data.NLP.LDA.LdaGibbsSampler.V">
 <summary>
 vocabulary size
 词表大小
 </summary> 
</member>
<member name="P:Microsoft.VisualBasic.Data.NLP.LDA.LdaGibbsSampler.K">
 <summary>
 number of topics
 主题数目
 </summary> 
</member>
<member name="F:Microsoft.VisualBasic.Data.NLP.LDA.LdaGibbsSampler.alpha">
 <summary>
 Dirichlet parameter (document--topic associations)
 文档——主题参数
 </summary> 
</member>
<member name="F:Microsoft.VisualBasic.Data.NLP.LDA.LdaGibbsSampler.beta">
 <summary>
 Dirichlet parameter (topic--term associations)
 主题——词语参数
 </summary>
</member>
<member name="F:Microsoft.VisualBasic.Data.NLP.LDA.LdaGibbsSampler.z">
 <summary>
 topic assignments for each word.
 每个词语的主题 z[i][j] := 文档i的第j个词语的主题编号
 </summary>
</member>
<member name="F:Microsoft.VisualBasic.Data.NLP.LDA.LdaGibbsSampler.nw">
 <summary>
 cwt[i][j] number of instances of word i (term?) assigned to topic j.
 计数器，nw[i][j] := 词语i归入主题j的次数
 </summary>
</member>
<member name="F:Microsoft.VisualBasic.Data.NLP.LDA.LdaGibbsSampler.nd">
 <summary>
 na[i][j] number of words in document i assigned to topic j.
 计数器，nd[i][j] := 文档[i]中归入主题j的词语的个数
 </summary> 
</member>
<member name="F:Microsoft.VisualBasic.Data.NLP.LDA.LdaGibbsSampler.nwsum">
 <summary>
 nwsum[j] total number of words assigned to topic j.
 计数器，nwsum[j] := 归入主题j词语的个数
 </summary>
</member>
<member name="F:Microsoft.VisualBasic.Data.NLP.LDA.LdaGibbsSampler.ndsum">
 <summary>
 nasum[i] total number of words in document i.
 计数器,ndsum[i] := 文档i中全部词语的数量
 </summary>
</member>
<member name="F:Microsoft.VisualBasic.Data.NLP.LDA.LdaGibbsSampler.thetasum">
 <summary>
 cumulative statistics of theta
 theta的累积量
 </summary>
</member>
<member name="F:Microsoft.VisualBasic.Data.NLP.LDA.LdaGibbsSampler.phisum">
 <summary>
 cumulative statistics of phi
 phi的累积量
 </summary>
</member>
<member name="F:Microsoft.VisualBasic.Data.NLP.LDA.LdaGibbsSampler.numstats">
 <summary>
 size of statistics
 样本容量
 </summary>
</member>
<member name="F:Microsoft.VisualBasic.Data.NLP.LDA.LdaGibbsSampler.THIN_INTERVAL">
 <summary>
 sampling lag (?)
 多久更新一次统计量
 </summary> 
</member>
<member name="F:Microsoft.VisualBasic.Data.NLP.LDA.LdaGibbsSampler.BURN_IN">
 <summary>
 burn-in period
 收敛前的迭代次数
 </summary>
</member>
<member name="F:Microsoft.VisualBasic.Data.NLP.LDA.LdaGibbsSampler.ITERATIONS">
 <summary>
 max iterations
 最大迭代次数
 </summary>
</member>
<member name="F:Microsoft.VisualBasic.Data.NLP.LDA.LdaGibbsSampler.SAMPLE_LAG">
 <summary>
 sample lag (if -1 only one sample taken)
 最后的模型个数（取收敛后的n个迭代的参数做平均可以使得模型质量更高）
 </summary>
</member>
<member name="P:Microsoft.VisualBasic.Data.NLP.LDA.LdaGibbsSampler.Theta">
 <summary>
 Retrieve estimated document--topic associations. If sample lag > 0 then
 the mean value of all sampled statistics for theta[][] is taken.
 获取文档——主题矩阵
 </summary>
 <returns> theta multinomial mixture of document topics (M x K) </returns>
</member>
<member name="P:Microsoft.VisualBasic.Data.NLP.LDA.LdaGibbsSampler.Phi">
 <summary>
 Retrieve estimated topic--word associations. If sample lag > 0 then the
 mean value of all sampled statistics for phi[][] is taken.
 获取主题——词语矩阵
 </summary>
 <returns> phi multinomial mixture of topic words (K x V) </returns>
</member>
<member name="M:Microsoft.VisualBasic.Data.NLP.LDA.LdaGibbsSampler.#ctor(System.Int32[][],System.Int32,System.Action{System.Object})">
 <summary>
 Initialise the Gibbs sampler with data.
 用数据初始化采样器
 </summary>
 <param name="documents"> 文档 </param>
 <param name="V">vocabulary size 词表大小 </param> 
</member>
<member name="M:Microsoft.VisualBasic.Data.NLP.LDA.LdaGibbsSampler.initialState(System.Int32)">
 <summary>
 Initialisation: Must start with an assignment of observations to topics ?
 Many alternatives are possible, I chose to perform random assignments
 with equal probabilities
 随机初始化状态
 </summary>
 <param name="K"> number of topics K个主题 </param>
</member>
<member name="M:Microsoft.VisualBasic.Data.NLP.LDA.LdaGibbsSampler.gibbs(System.Int32)">
 <summary>
 with default cutoff alpha=2.0 and beta=0.5
 </summary>
 <param name="K"></param>
</member>
<member name="M:Microsoft.VisualBasic.Data.NLP.LDA.LdaGibbsSampler.sampling(System.Int32)">
 <summary>
 for all z_i
 do chain data sampling
 </summary>
 <param name="zi"></param>
</member>
<member name="M:Microsoft.VisualBasic.Data.NLP.LDA.LdaGibbsSampler.gibbs(System.Int32,System.Double,System.Double)">
 <summary>
 Main method: Select initial state ? Repeat a large number of times: 1.
 Select an element 2. Update conditional on other elements. If
 appropriate, output summary for each run.
 采样
 </summary>
 <param name="K">     number of topics 主题数 </param>
 <param name="alpha"> symmetric prior parameter on document--topic associations 对称文档——主题先验概率？ </param>
 <param name="beta">  symmetric prior parameter on topic--term associations 对称主题——词语先验概率？ </param> 
</member>
<member name="M:Microsoft.VisualBasic.Data.NLP.LDA.LdaGibbsSampler.update_params">
 <summary>
 Add to the statistics the values of theta and phi for the current state.
 更新参数
 </summary>
</member>
<member name="M:Microsoft.VisualBasic.Data.NLP.LDA.LdaGibbsSampler.configure(System.Int32,System.Int32,System.Int32,System.Int32)">
 <summary>
 Configure the gibbs sampler
 配置采样器
 </summary>
 <param name="iterations">   number of total iterations </param>
 <param name="burnIn">       number of burn-in iterations </param>
 <param name="thinInterval"> update statistics interval </param>
 <param name="sampleLag">    sample interval (-1 for just one sample at the end) </param>
</member>
<member name="T:Microsoft.VisualBasic.Data.NLP.LDA.LdaInterpreter">
 <summary>
 @author hankcs
 </summary>
</member>
<member name="M:Microsoft.VisualBasic.Data.NLP.LDA.LdaInterpreter.translate(System.Double[][],Microsoft.VisualBasic.Data.NLP.LDA.Vocabulary,System.Int32)">
 <summary>
 To translate a LDA matrix to readable result </summary>
 <param name="phi"> the LDA model </param>
 <param name="vocabulary"> </param>
 <param name="limit"> limit of max words in a topic </param>
 <returns> a map array </returns>
</member>
<member name="M:Microsoft.VisualBasic.Data.NLP.LDA.LdaInterpreter.explain(System.Collections.Generic.IDictionary{System.String,System.Double}[],System.IO.TextWriter)">
 <summary>
 To print the result in a well formatted form </summary>
 <param name="result"> </param>
</member>
<member name="T:Microsoft.VisualBasic.Data.NLP.LDA.Vocabulary">
 <summary>
 a word set mapping between the word and its index id value
 
 @author hankcs
 </summary>
</member>
<member name="T:Microsoft.VisualBasic.Data.NLP.Stemmer">
 <summary>
 Stemmer, implementing the Porter Stemming Algorithm
  
 The Stemmer class transforms a word into its root form.  The input
 word can be provided a character at time (by calling add()), or at once
 by calling one of the various stem(something) methods.
 
 > https://github.com/meefen/npl-lab/tree/master
 </summary>
 <remarks>
 The Porter stemming algorithm is a process for removing the commoner morphological and inflexional endings 
 from words in English. Its main use is as part of a term normalization process that is usually done when 
 setting up Information Retrieval systems.
 
 **History and Development:**
 - The algorithm was developed by Martin Porter in 1980.
 - It has become one of the most popular stemming algorithms in English, largely due to its simplicity and effectiveness.
 
 **How it Works:**
 - The Porter stemmer works by applying a series of rules that remove or replace word endings. These rules are 
   applied in a specific order, and they are designed to handle different inflections and suffixes.
 - The rules are divided into steps, and each step contains a set of conditions and transformations. If a word meets a certain condition, a specific transformation is applied.
 - The algorithm uses a series of suffix replacement rules to reduce words to their root form. For example, "running" might be reduced to "run" by removing the "-ing" suffix.
 **Steps in the Algorithm:**
 1. **Plural Reduction:** Removes plural suffixes, like "-s" or "-es".
 2. **Past Tense Reduction:** Changes past tense endings, such as "-ed".
 3. **Adjective to Adverb Reduction:** Handles suffixes that convert adjectives to adverbs, like "-ly".
 4. **Progressive Reduction:** Deals with progressive tense endings, such as "-ing".
 5. **Suffix Reduction:** Applies a series of more complex rules to handle various other suffixes.
 **Advantages:**
 - **Simplicity:** The algorithm is relatively simple to implement and understand.
 - **Effectiveness:** It works well for a wide range of English words, reducing them to their base or root form.
 - **Broad Applicability:** It is used in various applications, including search engines, text analysis, and natural language processing.
 **Disadvantages:**
 - **Over-Stemming:** Sometimes it reduces words too aggressively, leading to over-stemming. For example, "university" 
   might be reduced to "univers", which is not a valid English word.
 - **Under-Stemming:** It may fail to reduce related words to the same stem. For example, "meeting" and "meet" might not be reduced to the same stem.
 - **Language Specific:** It is designed specifically for English and may not work well for other languages.
 **Applications:**
 - **Information Retrieval:** Improves the accuracy of search engines by reducing words to their base form, allowing for more effective matching of queries to documents.
 - **Text Mining:** Helps in analyzing large collections of text by reducing the dimensionality of the data.
 - **Natural Language Processing (NLP):** Used in various NLP tasks, such as text classification, clustering, and summarization.
 **Extensions and Variants:**
 - Over the years, several extensions and variants of the Porter stemmer have been developed to address its limitations 
   and improve its performance. These include the Snowball stemmer and the Porter2 stemmer.
 In summary, the Porter stemming algorithm is a foundational technique in text processing that helps in normalizing words 
 to their base form, thereby enhancing the efficiency and effectiveness of various text-based applications. Despite its
 limitations, it remains a widely used and influential algorithm in the field of computational linguistics and information 
 retrieval.
 </remarks>
</member>
<member name="M:Microsoft.VisualBasic.Data.NLP.Stemmer.add(System.Char)">
 <summary>
 Add a character to the word being stemmed.  When you are finished
 adding characters, you can call stem(void) to stem the word.
 </summary>
</member>
<member name="M:Microsoft.VisualBasic.Data.NLP.Stemmer.add(System.Char[],System.Int32)">
 <summary>
 Adds wLen characters to the word being stemmed contained in a portion
 of a char[] array. This is like repeated calls of add(char ch), but
 faster.
 </summary>
</member>
<member name="M:Microsoft.VisualBasic.Data.NLP.Stemmer.ToString">
 <summary>
 After a word has been stemmed, it can be retrieved by toString(),
 or a reference to the internal buffer can be retrieved by getResultBuffer
 and getResultLength (which is generally more efficient.)
 </summary>
</member>
<member name="P:Microsoft.VisualBasic.Data.NLP.Stemmer.ResultLength">
 <summary>
 Returns the length of the word resulting from the stemming process.
 </summary>
</member>
<member name="P:Microsoft.VisualBasic.Data.NLP.Stemmer.ResultBuffer">
 <summary>
 Returns a reference to a character buffer containing the results of
 the stemming process.  You also need to consult getResultLength()
 to determine the length of the result.
 </summary>
</member>
<member name="M:Microsoft.VisualBasic.Data.NLP.Stemmer.stem">
 <summary>
 Stem the word placed into the Stemmer buffer through calls to add().
 Returns true if the stemming process resulted in a word different
 from the input.  You can retrieve the result with
 getResultLength()/getResultBuffer() or toString().
 </summary>
</member>
<member name="T:Microsoft.VisualBasic.Data.NLP.DocumentTFIDF">
 <summary>
 TF-IDF algorithm: http://en.wikipedia.org/wiki/Tf*idf
 </summary>
 <remarks>
 **TF-IDF (Term Frequency-Inverse Document Frequency)** is a statistical measure that evaluates how relevant
 a word is to a document in a collection of documents. It is one of the most popular methods used for feature 
 extraction in text data. The TF-IDF score for a word increases proportionally to the number of times a word 
 appears in the document but is offset by the frequency of the word in the collection of documents.
 
 **Components of TF-IDF:**
 
 1. **Term Frequency (TF):** 
 
   - Measures how frequently a term occurs in a document. 
   - Calculated as the number of times a term appears in a document divided by the total number of terms in the document.
   - Formula: \( \text{TF}(t,d) = \frac{\text{Number of times term t appears in document d}}{\text{Total number of terms in document d}} \)
   
 2. **Inverse Document Frequency (IDF):**
 
   - Measures how important a term is to a collection of documents.
  - Calculated as the logarithm of the total number of documents divided by the number of documents containing the term.
   - Formula: \( \text{IDF}(t,D) = \log\left(\frac{\text{Total number of documents}}{\text{Number of documents containing term t}}\right) \)
   
 3. **TF-IDF Score:**
 
  - Combines TF and IDF to compute the TF-IDF score for a term in a document.
   - Formula: \( \text{TF-IDF}(t,d,D) = \text{TF}(t,d) \times \text{IDF}(t,D) \)
   
 **Interpretation:**
 
 - A high TF-IDF score indicates that the term is highly relevant to the document and rare across all documents.
 - A low TF-IDF score indicates that the term is either not very relevant to the document or very common across all documents.
 
 **Applications:**
 
 - **Information Retrieval:** Used to rank documents based on the relevance of search queries.
 - **Text Mining:** Helps in identifying important terms and features in text data.
 - **Machine Learning:** Used as a feature vector for text classification, clustering, and other NLP tasks.
 
 **Advantages:**
 
 - Simple and intuitive.
 - Effective in identifying important terms in a document.
 - Widely used and well-understood in the field of text analysis.
 
 **Disadvantages:**
 
 - Does not consider the semantic meaning of words.
 - Can be biased towards longer documents.
 - May not perform well with very rare or very common terms.
 
 **Example:**
 
 Consider a collection of documents about animals. The term "animal" might appear frequently in all documents, 
 resulting in a high TF but a low IDF, and thus a lower TF-IDF score. Conversely, a specific term like "giraffe"
 might appear less frequently but only in a few documents, resulting in a higher TF-IDF score, indicating its 
 relevance to those specific documents.
 
 In summary, TF-IDF is a powerful and widely-used technique for text analysis that helps in identifying the 
 importance of terms within a document relative to a collection of documents.
 </remarks>
</member>
<member name="F:Microsoft.VisualBasic.Data.NLP.DocumentTFIDF.stopwords">
 <summary>
 a common set of English stopwords
 </summary>
</member>
<member name="F:Microsoft.VisualBasic.Data.NLP.DocumentTFIDF.tfidf">
 <summary>
 the tf-idf algorithm engine
 </summary>
</member>
<member name="M:Microsoft.VisualBasic.Data.NLP.DocumentTFIDF.#ctor(System.String[],System.Collections.Generic.IEnumerable{System.String})">
 <summary>
 Constructor </summary>
 <param name="documents"> documents represented as strings </param>
 <remarks>
 document similarity measurement
 </remarks>
</member>
<member name="M:Microsoft.VisualBasic.Data.NLP.DocumentTFIDF.parseDocuments(System.String[])">
 <summary>
 Parse documents into bags of words </summary>
 <param name="docs"> documents in strings </param>
 <returns> a list of documents represented by bags of words </returns>
</member>
<member name="M:Microsoft.VisualBasic.Data.NLP.DocumentTFIDF.getSimilarity(System.Int32,System.Int32)">
 <summary>
 Get similarity score between two documents </summary>
 <param name="doc_i"> index of one document </param>
 <param name="doc_j"> index of another document </param>
 <returns> similarity score </returns>
</member>
<member name="M:Microsoft.VisualBasic.Data.NLP.DocumentTFIDF.getDocumentVector(System.Int32)">
 <summary>
 Compile a vector for a document </summary>
 <param name="docIndex"> index of a document </param>
 <returns> the vector representation of the document </returns>
</member>
<member name="F:Microsoft.VisualBasic.Data.NLP.TFIDF.vecs">
 <summary>
 存储 序列ID -> {K-mer: 出现次数}
 </summary>
</member>
<member name="P:Microsoft.VisualBasic.Data.NLP.TFIDF.N">
 <summary>
 get N sequence
 </summary>
 <returns></returns>
</member>
<member name="P:Microsoft.VisualBasic.Data.NLP.TFIDF.Words">
 <summary>
 get all unique words inside the sequence collection
 </summary>
 <returns></returns>
</member>
<member name="M:Microsoft.VisualBasic.Data.NLP.TFIDF.SetWords(System.Collections.Generic.IEnumerable{System.String})">
 <summary>
 One example: processing sequence embedding with kmers, set all alphabet combination of the kmer words for sequence
 (the input sequence kmer collection may be just a part subset of the input <paramref name="externalWords"/>). 
 so the generated vector could be comparable with the result of another batch of the sequence processing result.
 </summary>
 <param name="externalWords"></param>
 <returns></returns>
 <remarks>
 each time the <see cref="M:Microsoft.VisualBasic.Data.NLP.TFIDF.Add(System.String,System.Collections.Generic.IEnumerable{System.String})"/> method is call in this module, the words cache will be cleared. 
 this function should be called after all sequence has already been added into this generator.
 </remarks>
</member>
<member name="M:Microsoft.VisualBasic.Data.NLP.TFIDF.TfidfVectorizer(System.String)">
 <summary>
 
 </summary>
 <param name="id">the sequence id</param>
 <returns></returns>
</member>
<member name="M:Microsoft.VisualBasic.Data.NLP.TFIDF.TfidfVectorizer(System.Boolean)">
 <summary>
 generates the TF-IDF matrix
 </summary>
 <returns>
 A dataframe object with row is sequence and column is word data
 </returns>
</member>
<member name="M:Microsoft.VisualBasic.Data.NLP.TFIDF.OneHotVectorizer">
 <summary>
 n-gram One-hot(Bag-of-n-grams)
 </summary>
 <returns></returns>
</member>
<member name="M:Microsoft.VisualBasic.Data.NLP.TFIDF.DF(System.String)">
 <summary>
 
 </summary>
 <param name="v"></param>
 <returns></returns>
</member>
<member name="M:Microsoft.VisualBasic.Data.NLP.TFIDF.IDF(System.String)">
 <summary>
 IDF (Inverse Document Frequency)
 </summary>
 <param name="v"></param>
 <returns></returns>
</member>
</members>
</doc>
